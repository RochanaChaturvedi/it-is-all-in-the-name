{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 2class_models_original.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mtLMeGC7AOLX",
        "F7SWMbfuRSoB"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RochanaChaturvedi/it-is-all-in-the-name/blob/main/Two_class_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI6BGIJwj78n"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCey5dgV8IzL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXXVHkaNkbHR"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import pickle \n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import layers,initializers,regularizers,constraints\n",
        "from keras.layers import Layer\n",
        "import keras.initializers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten,Multiply,Add,Permute,Dot,Activation,\\\n",
        "    AlphaDropout,RepeatVector, Dense, Bidirectional,LSTM, BatchNormalization, SpatialDropout1D,Dropout,\\\n",
        "    GlobalMaxPool1D,GlobalAveragePooling1D,Concatenate,Input,Conv1D, MaxPool1D, Embedding,TimeDistributed,Lambda, Reshape,TimeDistributed\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras.models import load_model\n",
        "from keras.utils import plot_model\n",
        "import keras.backend as K\n",
        "\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold, KFold\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.feature_selection import RFE\n",
        "from itertools import compress\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "from deepexplain.tensorflow import DeepExplain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88-kNEn4Ho4s"
      },
      "source": [
        "# Global constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WadODWVVHrzK"
      },
      "source": [
        "eps=1e-20\n",
        "np.random.seed(42)\n",
        "\n",
        "base_dir= \"/content/drive/My Drive/name_to_religion/\"\n",
        "if not os.path.isdir(base_dir):\n",
        "    os.mkdir(base_dir)\n",
        "\n",
        "data_dir= base_dir+\"data/\"\n",
        "if not os.path.isdir(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "model_dir= base_dir+\"model/\"\n",
        "if not os.path.isdir(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "output_dir= data_dir+\"output/\"\n",
        "if not os.path.isdir(output_dir):\n",
        "    os.mkdir(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQo1p2uELQS6"
      },
      "source": [
        "# Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wINZrNR3b-o6"
      },
      "source": [
        "#Clean data- call once\n",
        "def clean_reds():\n",
        "  data=pd.read_csv(data_dir+\"REDS.csv\")\n",
        "  data.pname.replace(np.nan, '', regex=True,inplace=True)\n",
        "  data.name=data.name.str.upper()\n",
        "  data.pname=data.pname.str.upper()\n",
        "  data.name.replace(\"\\.\",\" \", regex=True,inplace=True)\n",
        "  data.pname.replace(\"\\.\",\" \", regex=True,inplace=True)\n",
        "  data.name.replace(\"\\s+\",\" \", regex=True,inplace=True)\n",
        "  data.pname.replace(\"\\s+\",\" \", regex=True,inplace=True)\n",
        "  data=data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "  data.sex.fillna(3,inplace=True)\n",
        "  data.to_csv(data_dir+\"REDS_cleaned.csv\",index=False)\n",
        "  \n",
        "clean_reds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qri9bhM4kZ_H"
      },
      "source": [
        "def getTrain_data():\n",
        "  data=pd.read_csv(data_dir+\"REDS_cleaned.csv\")    \n",
        "  data.name.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "  data.pname.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "  data.name=\"{\"+data.name.astype(str)+\"}\"\n",
        "  data.pname=\"{\"+data.pname.astype(str)+\"}\"\n",
        "  if concat_model:\n",
        "    if non_neural:\n",
        "      data.name='#'+data.name.astype(str)+'#'+data.pname.astype(str)+'#' \n",
        "    else:  \n",
        "      data.name='#'+data.name.astype(str)+'#'+data.pname.astype(str)+'#'\n",
        "  x=data.name\n",
        "  xp=data.pname\n",
        "  xi=data.index\n",
        "  y=data.muslim\n",
        "  \n",
        "  return(x,xi,y,xp)\n",
        "# (_,_,_,_,_)=getTrain_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmFj0i_hcVsj"
      },
      "source": [
        "def getDataSplit():\n",
        "  (x,_,y,xp)=getTrain_data()\n",
        "  (x_train1, x_test,xp_train1,xp_test, y_train1, y_test) = train_test_split(x, xp,y, stratify=y, random_state=42,test_size=0.1)\n",
        "  (x_train, x_val,xp_train,xp_val, y_train, y_val)= train_test_split(x_train1, xp_train1, y_train1,random_state=42, stratify=y_train1, test_size=0.1111)\n",
        "  return (x_train,x_val,x_test,xp_train,xp_val,xp_test,y_train,y_val,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEKkb3zHUGw-"
      },
      "source": [
        "def load_upsbm():\n",
        "  data_test_sbm = pd.read_csv(data_dir+\"upsbm_annotated.csv\")\n",
        "\n",
        "  data_test_sbm.familyhead=data_test_sbm.familyhead.str.upper()\n",
        "  data_test_sbm.fatherhusbandname=data_test_sbm.fatherhusbandname.str.upper()\n",
        "\n",
        "  data_test_sbm.familyhead.replace(\"\\.\",\" \", regex=True,inplace=True)\n",
        "  data_test_sbm.familyhead.replace(\"([A-Z])\\\\1\\\\1+\",\"\\\\1\", regex=True,inplace=True)\n",
        "  data_test_sbm.familyhead.replace(\"\\s+\",\" \", regex=True,inplace=True)\n",
        "\n",
        "  data_test_sbm.fatherhusbandname.replace(\"\\.\",\" \", regex=True,inplace=True)\n",
        "  data_test_sbm.fatherhusbandname.replace(\"([A-Z])\\\\1\\\\1+\",\"\\\\1\", regex=True,inplace=True)\n",
        "  data_test_sbm.fatherhusbandname.replace(\"\\s+\",\" \", regex=True,inplace=True)\n",
        "\n",
        "  data_test_sbm = data_test_sbm.apply(lambda x: x.str.strip(\" \") if x.dtype == \"object\" else x)\n",
        "  \n",
        "  data_test_sbm.familyhead.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "  data_test_sbm.fatherhusbandname.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "\n",
        "  y_sbm = data_test_sbm.muslim.map(lambda y:\"Muslim\" if y==1 else \"Others\")\n",
        "  x_sbm=\"{\"+data_test_sbm.familyhead.astype(str)+\"}\"\n",
        "  xp_sbm=\"{\"+data_test_sbm.fatherhusbandname.astype(str)+\"}\"\n",
        "  if(concat_model):\n",
        "    if non_neural:\n",
        "      x_sbm=\"#\"+x_sbm+\"#\"+xp_sbm+\"#\"  \n",
        "    else:\n",
        "      x_sbm=\"#\"+x_sbm+\"#\"+xp_sbm+\"#\"\n",
        "  \n",
        "  return x_sbm,xp_sbm,y_sbm\n",
        "\n",
        "# x_sbm,xp_sbm,y_sbm=load_upsbm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsr2yN6vLoeU"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjybXNsqdmhM"
      },
      "source": [
        "## non_neural: SVM Linear/logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tq5wI5TRxI2"
      },
      "source": [
        "non_neural=True\n",
        "eps=1e-20\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An5cW1eZU7D0"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYPtbugZg3fS"
      },
      "source": [
        "for classifier in [\"LOGIT\",\"SVM\"]:\n",
        "  for concat_model in[True,False]:\n",
        "    if classifier==\"LOGIT\":\n",
        "      if concat_model:\n",
        "        C=7.5                #Hyperparameters obtained by tuning on validation set\n",
        "        ngram=9\n",
        "      else:\n",
        "        C=6.95\n",
        "        ngram=5\n",
        "      clf = LogisticRegression(verbose=2, class_weight='balanced', random_state=42, n_jobs=-1, C=C)\n",
        "    else:\n",
        "      if concat_model:\n",
        "        C=.32\n",
        "        ngram=10\n",
        "      else:\n",
        "        C=.21\n",
        "        ngram=9\n",
        "      clf = svm.LinearSVC(verbose=2, class_weight='balanced', C=C)\n",
        "\n",
        "    x_train,x_val,x_test,xp_train,xp_val,xp_test,y_train,y_val,y_test =getDataSplit() \n",
        "    if concat_model==False:\n",
        "      d=pd.DataFrame({\"name\":x_train,\"muslim\":y_train})\n",
        "      data2=pd.DataFrame({\"name\":xp_train,\"muslim\":y_train})\n",
        "      data2=data2[data2.name!=\"\"]\n",
        "      data=pd.concat([d,data2],axis=0)\n",
        "      x_train=d.name\n",
        "      y_train=d.muslim\n",
        "        \n",
        "    vectorizer = TfidfVectorizer(min_df=5, max_df=.5, analyzer='char', ngram_range=(1, ngram))#max_features=40000\n",
        "    vectorizer.fit(x_train)\n",
        "    features=vectorizer.get_feature_names()\n",
        "    tfidf_matrix_train = vectorizer.transform(x_train)\n",
        "    tfidf_matrix_val = vectorizer.transform(x_val)\n",
        "    tfidf_matrix_test = vectorizer.transform(x_test)\n",
        "    clf.fit(tfidf_matrix_train, y_train)\n",
        "    pickle.dump(clf,open(model_dir+'model_2class_'+classifier+'_concat_'+str(concat_model)+'.sav',\"wb\"))\n",
        "    pickle.dump(vectorizer,open(model_dir+'vectorizer_2class_'+classifier+'_concat_'+str(concat_model)+'.sav',\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxWoWClOVCwm"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKQM1e0bBlq2"
      },
      "source": [
        "for classifier in [\"LOGIT\",\"SVM\"]:\n",
        "  for concat_model in [True,False]:\n",
        "    print(classifier,concat_model)\n",
        "\n",
        "    x_sbm,xp_sbm,y_sbm=load_upsbm()\n",
        "    _,x_val,x_test,_,xp_val,xp_test,_,y_val,y_test =getDataSplit() \n",
        "\n",
        "\n",
        "    vectorizer = pickle.load(open(model_dir+'vectorizer_2class_'+classifier+'_concat_'+str(concat_model)+'.sav','rb'))\n",
        "    clf=pickle.load(open(model_dir+'model_2class_'+classifier+'_concat_'+str(concat_model)+'.sav','rb'))\n",
        "  \n",
        "    tfidf_matrix_val = vectorizer.transform(x_val)\n",
        "    tfidf_matrix_test = vectorizer.transform(x_test)\n",
        "    tfidf_matrix_sbm= vectorizer.transform(x_sbm)\n",
        "\n",
        "#reds val\n",
        "    y_pred_val=clf.predict(tfidf_matrix_val)\n",
        "    df=pd.DataFrame({\"familyhead\":x_val,\"parent\":xp_val,\"religion\":y_val,\n",
        "                 \"predicted\":pd.Series(y_pred_val)})\n",
        "    df.to_csv(data_dir+'val_predictions_2class_'+classifier+'_concat_'+str(concat_model)+'.csv')\n",
        "    print('REDS validation Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(y_val,y_pred_val)))\n",
        "    print(classification_report(y_val,y_pred_val,digits=4))\n",
        "    conf_mat = confusion_matrix(y_val,y_pred_val)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "                xticklabels={\"nonMuslim\",\"Muslim\"}, yticklabels={\"nonMuslim\",\"Muslim\"})\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "#reds_test\n",
        "    y_pred_test=clf.predict(tfidf_matrix_test)\n",
        "    df=pd.DataFrame({\"familyhead\":x_test,\"parent\":xp_test,\"religion\":y_test,\n",
        "                 \"predicted\":pd.Series(y_pred_test)})\n",
        "    df.to_csv(data_dir+'test_predictions_2class_'+classifier+'_concat_'+str(concat_model)+'.csv')\n",
        "\n",
        "    print('REDS test Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(y_test,y_pred_test)))\n",
        "    print(classification_report(y_test,y_pred_test,digits=4))\n",
        "    conf_mat = confusion_matrix(y_test,y_pred_test)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "                xticklabels={\"nonMuslim\",\"Muslim\"}, yticklabels={\"nonMuslim\",\"Muslim\"})\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "#UPSBM    \n",
        "    y_pred_sbm=clf.predict(tfidf_matrix_sbm)\n",
        "    df=pd.DataFrame({\"familyhead\":x_sbm,\"parent\":xp_sbm,\"religion\":y_sbm,\n",
        "                 \"predicted\":pd.Series(y_pred_sbm)})\n",
        "\n",
        "    df.to_csv(data_dir+'sbm_predictions_2class_'+classifier+'_concat_'+str(concat_model)+'.csv')\n",
        "\n",
        "    print('SBM Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(y_sbm==\"Muslim\",y_pred_sbm)))\n",
        "    print(classification_report(y_sbm==\"Muslim\",y_pred_sbm,digits=4))\n",
        "    conf_mat = confusion_matrix(y_sbm==\"Muslim\",y_pred_sbm)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "                xticklabels={\"nonMuslim\",\"Muslim\"}, yticklabels={\"nonMuslim\",\"Muslim\"})\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iwM_HBTTzA6"
      },
      "source": [
        "### stage2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeoFwpTRtwDv"
      },
      "source": [
        "# classifier =\"LOGIT\"\n",
        "classifier=\"SVM\"\n",
        "concat_model= False\n",
        "vectorizer = pickle.load(open(model_dir+'vectorizer_2class_'+classifier+'_concat_'+str(concat_model)+'.sav','rb'))\n",
        "clf=pickle.load(open(model_dir+'model_2class_'+classifier+'_concat_'+str(concat_model)+'.sav','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xtFPYQaTDJ4"
      },
      "source": [
        "x_train,x_val,x_test,xp_train,xp_val,xp_test, y_train,y_val,y_test =getDataSplit() \n",
        "x_sbm,xp_sbm,y_sbm=load_upsbm()\n",
        "\n",
        "tfidf_matrix_val = vectorizer.transform(x_val)\n",
        "tfidf_matrix_test = vectorizer.transform(x_test)\n",
        "tfidf_matrix_sbm= vectorizer.transform(x_sbm)\n",
        "tfidf_matrix_train = vectorizer.transform(x_train)\n",
        "tfidf_matrix_train_p=vectorizer.transform(xp_train)\n",
        "tfidf_matrix_val_p=vectorizer.transform(xp_val)\n",
        "tfidf_matrix_test_p=vectorizer.transform(xp_test)\n",
        "tfidf_matrix_sbm_p= vectorizer.transform(xp_sbm)\n",
        "\n",
        "\n",
        "if classifier==\"LOGIT\":\n",
        "  y_pred_train=clf.predict_proba(tfidf_matrix_train)[:,1]\n",
        "  y_pred_test=clf.predict_proba(tfidf_matrix_test)[:,1]\n",
        "  y_pred_val=clf.predict_proba(tfidf_matrix_val)[:,1]\n",
        "  y_pred_sbm=clf.predict_proba(tfidf_matrix_sbm)[:,1]\n",
        "  y_pred_train=clf.predict_proba(tfidf_matrix_train)[:,1]\n",
        "  pred_p_train=clf.predict_proba(tfidf_matrix_train_p)[:,1]\n",
        "  pred_p_val=clf.predict_proba(tfidf_matrix_val_p)[:,1]\n",
        "  pred_p_test=clf.predict_proba(tfidf_matrix_test_p)[:,1]\n",
        "  pred_p_sbm=clf.predict_proba(tfidf_matrix_sbm_p)[:,1]\n",
        "else:\n",
        "  y_pred_train=clf.decision_function(tfidf_matrix_train)\n",
        "  y_pred_test=clf.decision_function(tfidf_matrix_test)\n",
        "  y_pred_val=clf.decision_function(tfidf_matrix_val)\n",
        "  y_pred_sbm=clf.decision_function(tfidf_matrix_sbm)\n",
        "  y_pred_train=clf.decision_function(tfidf_matrix_train)\n",
        "  pred_p_train=clf.decision_function(tfidf_matrix_train_p)\n",
        "  pred_p_val=clf.decision_function(tfidf_matrix_val_p)\n",
        "  pred_p_test=clf.decision_function(tfidf_matrix_test_p)\n",
        "  pred_p_sbm=clf.decision_function(tfidf_matrix_sbm_p)  \n",
        "\n",
        "max_train=np.maximum(y_pred_train,pred_p_train)\n",
        "log_x_train=np.log(np.clip(y_pred_train, eps, 1 - eps))\n",
        "log_xp_train=np.log(np.clip(pred_p_train, eps, 1 - eps))\n",
        "log_max_train=np.maximum(log_x_train,log_xp_train)\n",
        "prod_train=np.multiply(y_pred_train,pred_p_train)\n",
        "xlogp_train=np.multiply(y_pred_train,log_xp_train)\n",
        "plogx_train=np.multiply(pred_p_train,log_x_train)\n",
        "\n",
        "max_val=np.maximum(y_pred_val,pred_p_val)\n",
        "log_x_val=np.log(np.clip(y_pred_val, eps, 1 - eps))\n",
        "log_xp_val=np.log(np.clip(pred_p_val, eps, 1 - eps))\n",
        "log_max_val=np.maximum(log_x_val,log_xp_val)\n",
        "prod_val=np.multiply(y_pred_val,pred_p_val)\n",
        "xlogp_val=np.multiply(y_pred_val,log_xp_val)\n",
        "plogx_val=np.multiply(pred_p_val,log_x_val)\n",
        "\n",
        "log_x_test=np.log(np.clip(y_pred_test, eps, 1 - eps))\n",
        "log_xp_test=np.log(np.clip(pred_p_test, eps, 1 - eps))\n",
        "log_max_test=np.maximum(log_x_test,log_xp_test)\n",
        "max_test=np.maximum(y_pred_test,pred_p_test)\n",
        "prod_test=np.multiply(y_pred_test,pred_p_test)\n",
        "xlogp_test=np.multiply(y_pred_test,log_xp_test)\n",
        "plogx_test=np.multiply(pred_p_test,log_x_test)\n",
        "\n",
        "log_x_sbm=np.log(np.clip(y_pred_sbm, eps, 1 - eps))\n",
        "log_xp_sbm=np.log(np.clip(pred_p_sbm, eps, 1 - eps))\n",
        "log_max_sbm=np.maximum(log_x_sbm,log_xp_sbm)\n",
        "max_sbm=np.maximum(y_pred_sbm,pred_p_sbm)\n",
        "prod_sbm=np.multiply(y_pred_sbm,pred_p_sbm)\n",
        "xlogp_sbm=np.multiply(y_pred_sbm,log_xp_sbm)\n",
        "plogx_sbm=np.multiply(pred_p_sbm,log_x_sbm)\n",
        "\n",
        "\n",
        "features_train=(y_pred_train,pred_p_train, max_train,log_x_train,log_xp_train,log_max_train,\n",
        "                      prod_train, plogx_train,xlogp_train)\n",
        "features_test=(y_pred_test,pred_p_test, max_test,log_x_test,log_xp_test,log_max_test,\n",
        "                      prod_test, plogx_test,xlogp_test)\n",
        "features_val=(y_pred_val,pred_p_val, max_val,log_x_val,log_xp_val,log_max_val,\n",
        "                      prod_val, plogx_val,xlogp_val)\n",
        "features_sbm=(y_pred_sbm,pred_p_sbm, max_sbm,log_x_sbm,log_xp_sbm,log_max_sbm,\n",
        "                      prod_sbm, plogx_sbm, xlogp_sbm)\n",
        "\n",
        "X_train=pd.DataFrame(np.column_stack(features_train))\n",
        "X_val=pd.DataFrame(np.column_stack(features_val))\n",
        "X_test=pd.DataFrame(np.column_stack(features_test))\n",
        "X_sbm=pd.DataFrame(np.column_stack(features_sbm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T4mkKfK9O2G"
      },
      "source": [
        "opt = 2\n",
        "score=0\n",
        "classifier_1 =  svm.LinearSVC(verbose=0,class_weight='balanced', C=.4,max_iter=2000)\n",
        "\n",
        "for i in range(2,10):   \n",
        "  sel = RFE(classifier_1, i) \n",
        "  sel = sel.fit(X_train, y_train)\n",
        "  X_train_=pd.DataFrame(np.column_stack(tuple(compress(features_train,sel.support_))))\n",
        "  X_val_=pd.DataFrame(np.column_stack(tuple(compress(features_val,sel.support_))))\n",
        "\n",
        "  classifier_1.fit(X_train_, list(y_train))\n",
        "  pred_val=classifier_1.predict(X_val_)\n",
        "  rec_score=recall_score(y_val, pred_val,average='macro')\n",
        "  if rec_score>score:\n",
        "    score=rec_score\n",
        "    opt=i\n",
        "    rfe=sel\n",
        "  \n",
        "rfe = RFE(classifier_1, opt)\n",
        "rfe = rfe.fit(X_train, y_train)\n",
        "print(rfe.support_)\n",
        "selected=rfe.support_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N68JKWZWa9h"
      },
      "source": [
        "X_train_=pd.DataFrame(np.column_stack(tuple(compress(features_train,selected))))\n",
        "X_val=pd.DataFrame(np.column_stack(tuple(compress(features_val,selected))))\n",
        "X_test=pd.DataFrame(np.column_stack(tuple(compress(features_test,selected))))\n",
        "X_sbm=pd.DataFrame(np.column_stack(tuple(compress(features_sbm,selected))))\n",
        "\n",
        "classifier_1.fit(X_train_, list(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8jal089CRVo"
      },
      "source": [
        "filename = model_dir+'2class_level2_'+classifier+'.sav'\n",
        "pickle.dump((rfe,classifier_1), open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFVRlr8s1r79"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zQxsMdu1ul6"
      },
      "source": [
        "pred2_val = classifier_1.predict(X_val)\n",
        "print('Accuracy of LEVEL 2 on VAL set: {:.2f}'.format(accuracy_score(y_val, pred2_val)))\n",
        "print(classification_report(y_val, pred2_val, digits=4))\n",
        "\n",
        "\n",
        "pred2_test = classifier_1.predict(X_test)\n",
        "print('Accuracy of LEVEL 2 on REDS test set: {:.2f}'.format(accuracy_score(y_test, pred2_test)))\n",
        "print(classification_report(y_test, pred2_test, digits=4))\n",
        "\n",
        "\n",
        "pred2_sbm = classifier_1.predict(X_sbm)\n",
        "print('Accuracy of LEVEL 2 on SBM test set: {:.2f}'.format(accuracy_score(y_sbm==\"Muslim\", pred2_sbm)))\n",
        "print(classification_report(y_sbm==\"Muslim\", pred2_sbm, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E060TugtSCNN"
      },
      "source": [
        "## Neural Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIO4MhwtAVnt"
      },
      "source": [
        "def highway_layers(value, n_layers,initializer, activation=\"elu\", gate_bias=-2):\n",
        "    \n",
        "    gate_bias_initializer = keras.initializers.Constant(gate_bias)\n",
        "    for i in range(n_layers):     \n",
        "        dim = K.int_shape(value)[-1]\n",
        "        # print(dim)\n",
        "        carry = Dense(units=dim, kernel_initializer=initializer,bias_initializer=gate_bias_initializer)(value)#carry gate\n",
        "        carry = Activation(activation)(carry)\n",
        "        negated_gate = Lambda(lambda x: 1.0 - x,output_shape=(dim,))(carry)\n",
        "        transformed = Dense(units=dim,kernel_initializer=initializer)(value)\n",
        "        transformed = Activation(activation)(transformed)\n",
        "        transformed_gated = Multiply()([carry, transformed])#carry\n",
        "        identity_gated = Multiply()([negated_gate, value])\n",
        "        value = Concatenate()([transformed_gated, identity_gated])\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSKgLRLKLn_m"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg0-4PwaosXh"
      },
      "source": [
        "def final_LSTMmodel(shape, embedding_dim=20, drop_embed=0.1, dropout=.15, units=100, initializer='he_uniform', optimizer='Nadam'):\n",
        "  x_can = Input(shape=(shape[0],), dtype='int32')\n",
        "  embedding = Embedding(shape[1], embedding_dim, input_length=shape[0], trainable=True, embeddings_initializer=initializer)(x_can)\n",
        "  embedding = Dropout(drop_embed)(embedding)\n",
        "  embedding = SpatialDropout1D(drop_embed)(embedding) \n",
        "  x = CuDNNLSTM(units, kernel_initializer=initializer, return_sequences=True)(embedding)\n",
        "  x = TimeDistributed(Dense(1))(x)\n",
        "  x = GlobalMaxPool1D()(x)\n",
        "  x= highway_layers(x,1,initializer)\n",
        "  x = Dropout(dropout)(x)  \n",
        "  x = Dense(1, activation=\"sigmoid\",kernel_initializer=initializer)(x)\n",
        "  model = Model(x_can,x)\n",
        "  model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tCUvdi0Ltl4"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saIuJYsK770t"
      },
      "source": [
        "def final_CNNmodel(shape, embedding_dim=20, activation='tanh', drop_embed=0.1, dropout=.25, \n",
        "                       kernel_sizes=[[1,150], [2,200], [3,250], [4, 300], [5,300], [6,300]], initializer='he_uniform',\n",
        "                       optimizer='Nadam'):\n",
        "  x_can = Input(shape=(shape[0],), dtype='int32')\n",
        "  embedding = Embedding(shape[1], embedding_dim, input_length=shape[0], trainable=True, embeddings_initializer=initializer)(x_can)\n",
        "  embedding = Dropout(drop_embed)(embedding)\n",
        "\n",
        "  embedding = SpatialDropout1D(drop_embed)(embedding)\n",
        "  convs = []\n",
        "  \n",
        "  for [kernel_size,num_filter] in kernel_sizes:\n",
        "    l_conv = Conv1D(filters=num_filter, kernel_initializer=initializer, kernel_size=kernel_size,\n",
        "                    padding='valid', name=\"Convolution\"+str(kernel_size), activation=activation, strides=1)(embedding)\n",
        "    l_pool= GlobalMaxPool1D()(l_conv)\n",
        "    l_pool=AlphaDropout(.05)(l_pool)\n",
        "    \n",
        "    convs.append(l_pool)\n",
        "  x = Concatenate(axis=1)(convs)\n",
        "  x = Dropout(dropout)(x)  \n",
        "  x = Dense(1, activation=\"sigmoid\",kernel_initializer=initializer)(x)\n",
        "  model = Model(x_can,x)\n",
        "  model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnc5jjgtB0yr"
      },
      "source": [
        "### CLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsU7q6TgxAir"
      },
      "source": [
        "def final_CLSTMmodel(shape, embedding_dim=20, drop_embed=0.1, dropout=.25, units=250,\n",
        "                       kernel_sizes=[[2,225],[3,275],[4,350],[5,350],[6,350]], initializer='he_uniform', optimizer='Nadam'):\n",
        "  x_can = Input(shape=(shape[0],), dtype='int32')\n",
        "  embedding = Embedding(shape[1], embedding_dim, input_length=shape[0], trainable=True, embeddings_initializer=initializer)(x_can)\n",
        "  embedding = BatchNormalization()(embedding)\n",
        "  embedding = SpatialDropout1D(drop_embed)(embedding)\n",
        "  sentence_encoder = CuDNNLSTM(units, kernel_initializer=initializer,name=\"uni-LSTM\", return_sequences=True)(embedding)\n",
        "  sentence_encoder = TimeDistributed(Dense(1))(sentence_encoder)\n",
        "  sentence_encoder = GlobalMaxPool1D()(sentence_encoder)\n",
        "  sentence_encoder = Dropout(dropout)(sentence_encoder)\n",
        "  convs = []\n",
        "  convs.append(sentence_encoder)\n",
        "  \n",
        "  for [kernel_size,num_filter] in kernel_sizes:\n",
        "    l_conv = Conv1D(filters=num_filter, kernel_initializer=initializer, kernel_size=kernel_size,\n",
        "                    padding='valid', name=\"Convolution\"+str(kernel_size), activation='tanh')(embedding)\n",
        "    l_pool= MaxPool1D(kernel_size-1)(l_conv)\n",
        "    l_pool=AlphaDropout(.05)(l_pool)\n",
        "    sentence_encoder = CuDNNLSTM(units, kernel_initializer=initializer, return_sequences=True)(l_pool)\n",
        "    sentence_encoder = TimeDistributed(Dense(1))(sentence_encoder)\n",
        "    sentence_encoder = GlobalMaxPool1D()(sentence_encoder)\n",
        "    sentence_encoder =Dropout(dropout)(sentence_encoder)\n",
        "    convs.append(sentence_encoder)\n",
        "\n",
        "  x = Concatenate(axis=1)(convs)\n",
        "  x= highway_layers(x,1,initializer)\n",
        "  x = Dropout(dropout)(x)  \n",
        "  x = Dense(1, activation=\"sigmoid\",kernel_initializer=initializer)(x)\n",
        "  model = Model(x_can,x)\n",
        "  model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EI0_Ayo8MUt"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnbLpmPiq775"
      },
      "source": [
        "def tokenize(x,max_char):\n",
        "  unique_symbols = Counter()\n",
        "\n",
        "  for _, name in x.iteritems():\n",
        "      unique_symbols.update(name)\n",
        "\n",
        "  num_unique_symbols = len(unique_symbols) #- len(uncommon_symbols) + 1 \n",
        "  print(\"Unique symbols:\",num_unique_symbols, unique_symbols)\n",
        "\n",
        "  tokenizer = Tokenizer(\n",
        "      char_level=True,\n",
        "      filters=None,\n",
        "      oov_token='#',\n",
        "      lower=False,\n",
        "      num_words=None\n",
        "  )\n",
        "\n",
        "  tokenizer.fit_on_texts([x])\n",
        "  char_dict = {}\n",
        "  ind_to_char={}\n",
        "  for i, char in enumerate(unique_symbols):\n",
        "    char_dict[char] = i + 1\n",
        "    ind_to_char[i+1]=char\n",
        "  tokenizer.word_index = char_dict.copy()\n",
        "  print(char_dict)\n",
        "  \n",
        "  tokenizer.word_index[tokenizer.oov_token] = max(char_dict.values()) + 1\n",
        "  ind_to_char[max(char_dict.values()) + 1]=tokenizer.oov_token\n",
        "  max_char=max(map(len,x))\n",
        "  x_sequences = tokenizer.texts_to_sequences(x)\n",
        "  with open(data_dir+\"tokenizer\"+str(concat_model)+\".pkl\", \"wb\") as f:\n",
        "      pickle.dump((tokenizer,max_char), f)\n",
        "  \n",
        "  return (num_unique_symbols+1,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56VolPZQ8xHx"
      },
      "source": [
        "def getMaxchar():\n",
        "  data=pd.read_csv(data_dir+\"REDS_cleaned.csv\")\n",
        "  # data.name=data.name.str.upper()\n",
        "  # data.name.replace(\"\\.\",\" \", regex=True,inplace=True)\n",
        "  # data.name.replace(\"\\s+\",\" \", regex=True,inplace=True)\n",
        "  # data=data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "  data=pd.DataFrame({\"index\":data.index,\"name\":data.name,\n",
        "                     \"pname\":data.pname,\n",
        "                     \"religion\":data.religion,\"muslim\":data.muslim, \"sex\":data.sex})\n",
        "  data.pname.replace(np.nan, '', regex=True,inplace=True)\n",
        "\n",
        "  data.name.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "  data.name=\"{\"+data.name.astype(str)+\"}\"\n",
        "  data.pname.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "  data.pname=\"{\"+data.pname.astype(str)+\"}\"  \n",
        "  if(concat_model):    \n",
        "    data.name='#'+data.name.astype(str)+\"#\"+data.pname.astype(str)+\"#\"\n",
        "  max_char=max(map(len,data.name))\n",
        "  vocab_size,tokenizer=tokenize(data.name,max_char)\n",
        "  return max_char,tokenizer,vocab_size\n",
        "max_char,tokenizer,vocab_size=getMaxchar()\n",
        "print(max_char,vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7q7N4G_o1m2"
      },
      "source": [
        "def plot_hist(history):\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.plot(history['loss'], label='train_loss')\n",
        "  plt.plot(history['val_loss'], label='test_loss')\n",
        "  plt.title('Training and validation Loss')\n",
        "  plt.legend()\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.plot(history['acc'], label='train_Accuracy')\n",
        "  plt.plot(history['val_acc'], label='test_Accuracy')\n",
        "  plt.title('Training and validation Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JhzKprl1Gpg"
      },
      "source": [
        "# Utility function for Level2\n",
        "def map_logreg_features(X1,X2,support_):\n",
        "  log_pred=np.log(np.clip(X1, eps, 1 - eps))\n",
        "  log_pred_p=np.log(np.clip(X2, eps, 1 - eps))\n",
        "  log_max=np.maximum(log_pred,log_pred_p)\n",
        "  max_pred=np.maximum(X1,X2)\n",
        "  prod_pred=np.multiply(X1,X2)\n",
        "  ratio_pred=np.maximum(np.divide(X1,np.clip(X2, eps, 1 - eps)),\n",
        "                              np.divide(X2,np.clip(X1, eps, 1 - eps)))\n",
        "  plogx=np.multiply(X2,log_pred)\n",
        "  xlogp=np.multiply(X1,log_pred_p)\n",
        "\n",
        "  x=(X1,X2, max_pred,log_pred,log_pred_p,log_max,\n",
        "                      prod_pred, ratio_pred,plogx,xlogp)\n",
        "\n",
        "  x_col=['pred','pred_p', 'max_pred','log_pred','log_pred_p','log_max',\n",
        "                      'prod_pred', 'ratio_pred','plogx','xlogp']\n",
        "                      \n",
        "  res=pd.DataFrame(np.column_stack(tuple(compress(x,support_))),\n",
        "                   columns = list(compress(x_col,support_)))\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-5x83oIaPlc"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7spdSbIElE1a"
      },
      "source": [
        "#### Level 0 Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLgrvD-S3FnE"
      },
      "source": [
        "def trainCLSTMModel(model_fn,model_name,initial_epoch=0,epoch=25,model=None):\n",
        "  x_train,x_val,_,xp_train,xp_val,_,_,_,_,y_train,y_val,_=getDataSplit()\n",
        "  \n",
        "  if concat_model==False:\n",
        "    df1=pd.DataFrame({\"x\":x_train,\"y\":y_train})\n",
        "    df2=pd.DataFrame({\"x\":xp_train,\"y\":y_train})\n",
        "    df2 = df2[df2.x!=\"\"]\n",
        "    df=pd.concat([df1,df2],axis=0)\n",
        "\n",
        "    dfv1=pd.DataFrame({\"x\":x_val,\"y\":y_val})\n",
        "    dfv2=pd.DataFrame({\"x\":xp_val,\"y\":y_val})\n",
        "    dfv2 = dfv2[dfv2.x!=\"\"]\n",
        "    dfv=pd.concat([dfv1,dfv2],axis=0)\n",
        "    x_train=df.x\n",
        "    x_val=dfv.x\n",
        "    y_train=df.y\n",
        "    y_val=dfv.y\n",
        "    \n",
        "  padded_sequences_x=tokenizer.texts_to_sequences(x_train)\n",
        "  xtrain=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "  \n",
        "  padded_sequences_x=tokenizer.texts_to_sequences(x_val)\n",
        "  xval=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "  class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train), y_train)\n",
        "\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8,patience=5, min_lr=0.0003)\n",
        "  \n",
        "  mc = ModelCheckpoint(model_name,\n",
        "                       monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
        "  shape=(xtrain.shape[1], vocab_size)\n",
        "  if model==None:\n",
        "    model=model_fn(shape,vocab_size)\n",
        "  history=model.fit(xtrain, y_train,validation_data=(xval, y_val),initial_epoch =initial_epoch, epochs =epoch,\n",
        "                      shuffle=True, batch_size = 512,verbose=1,callbacks=[mc,reduce_lr],class_weight=class_weights,\n",
        "                      workers=-1, use_multiprocessing=True)\n",
        "  \n",
        "  model=load_model(model_name)\n",
        "  return (model, history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygqj6u7XZG1d"
      },
      "source": [
        "# model_name='LSTM' if concat_model==False else 'LSTM_concat\n",
        "# model_fn=final_LSTMmodel\n",
        "\n",
        "# model_name='CNN' if concat_model==False else 'CNN_concat\n",
        "# model_fn=final_CNNmodel\n",
        "\n",
        "# model_name='CNNLSTM' if concat_model==False else 'CNNLSTM_concat'\n",
        "# model_fn=final_CLSTMmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMNuDvGVaS5i"
      },
      "source": [
        "%%time\n",
        "model, history = trainCLSTMModel(model_fn,model_dir+'final_model_'+model_name+'.h5', 0, 20)\n",
        "plot_hist(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNuwJpCadn_q"
      },
      "source": [
        "#### Level 1 neural\n",
        "<i> Run only if concat_model=False </i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCXyP4jqal3N"
      },
      "source": [
        "#### Load level 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFqeifJxNyzU"
      },
      "source": [
        "model=load_model(model_dir+model_name+'.h5'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI-VJhdIdqUZ"
      },
      "source": [
        "#### Train ensemble/ Meta model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLfpZRWKdina"
      },
      "source": [
        "(x_train,x_val,x_test,xp_train,xp_val,xp_test,y_train,y_val,y_test)=getDataSplit()\n",
        "\n",
        "x_train=tokenizer.texts_to_sequences(x_train)\n",
        "x_train=pad_sequences(x_train, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "x_val=tokenizer.texts_to_sequences(x_val)\n",
        "x_val=pad_sequences(x_val, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "# %%time\n",
        "pred_train=model.predict(x_train,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "pred_val=model.predict(x_val,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "\n",
        "xp_train=tokenizer.texts_to_sequences(xp_train)\n",
        "xp_train=pad_sequences(xp_train, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "xp_val=tokenizer.texts_to_sequences(xp_val)\n",
        "xp_val=pad_sequences(xp_val, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "# %%time\n",
        "predp_train=model.predict(xp_train,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "predp_val=model.predict(xp_val,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "\n",
        "\n",
        "X_train=map_logreg_features(pred_train,predp_train,[True]*9)\n",
        "X_val=map_logreg_features(pred_val,predp_val,[True]*9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H33YawNDeWh3"
      },
      "source": [
        "##### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_wewrPGeV53"
      },
      "source": [
        "opt = 2\n",
        "score=0\n",
        "classifier_1 =  svm.LinearSVC(verbose=0, class_weight='balanced', C=.4, max_iter=2000, random_state=42)\n",
        "for i in range(2,10):   \n",
        "  sel = RFE(classifier_1, i) \n",
        "  sel = sel.fit(X_train, y_train)\n",
        "  X_train_=map_logreg_features(pred_train,predp_train,sel.support_)\n",
        "  X_val_=map_logreg_features(pred_val,predp_val,sel.support_)\n",
        "  classifier_1.fit(X_train_, list(y_train))\n",
        "  pred2_val=classifier_1.predict(X_val_)\n",
        "  rec_score=recall_score(y_val, pred2_val,average='macro')\n",
        "  if rec_score>score:\n",
        "    score=rec_score\n",
        "    opt=i\n",
        "    rfe=sel\n",
        "  \n",
        "rfe = RFE(classifier_1, opt)\n",
        "rfe = rfe.fit(X_train, y_train)\n",
        "selected=rfe.support_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1JMYTtmojx8"
      },
      "source": [
        "X_train_=map_logreg_features(pred_train,predp_train,rfe.support_)\n",
        "X_val_=map_logreg_features(pred_val,predp_val,rfe.support_)\n",
        "classifier_1.fit(X_train_, list(y_train))\n",
        "pred2_val = classifier_1.predict(X_val_)\n",
        "print('Accuracy of LEVEL 2 on test set: {:.2f}'.format(accuracy_score(y_val, pred2_val)))\n",
        "print(classification_report(y_val, pred2_val, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDRGxbKSwdJd"
      },
      "source": [
        "##### Save meta learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxnRIzyzAz9K"
      },
      "source": [
        "# Save the trained model as a pickle string. \n",
        "filename = model_dir+'stage2_'+model_name+'.sav'\n",
        "pickle.dump(classifier_1, open(filename, 'wb'))\n",
        "filename = model_dir+'rfe_'+model_name+'.sav'\n",
        "pickle.dump(rfe, open(filename, 'wb'))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIYWQBXjQjI6"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4zBZF8qEtgt"
      },
      "source": [
        "#### Load models, other objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28SysI0sCmaH"
      },
      "source": [
        "concat_model=False\n",
        "# model_name='LSTM' if concat_model==False else 'LSTM_concat'\n",
        "model_name='CNN' if concat_model==False else 'CNN_concat'\n",
        "# model_name='CNNLSTM' if concat_model==False else 'CNNLSTM_concat'\n",
        "model_path=model_dir+model_name+'.h5'\n",
        "model = load_model(model_path,custom_objects={\"f1\":f1})\n",
        "if concat_model==False:\n",
        "  logreg = pickle.load(open(model_dir+'stage2_'+model_name+'.sav','rb'))\n",
        "  rfe = pickle.load(open(model_dir+'rfe_'+model_name+'.sav','rb'))\n",
        "with open(data_dir+\"tokenizer\"+str(concat_model)+\".pkl\", \"rb\") as f:\n",
        "    (tokenizer,max_char)=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuNp8L-Szh8o"
      },
      "source": [
        "#### REDS TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBohxGr2GES1"
      },
      "source": [
        "_,_,x_test,_,_,xp_test,_,_,y_test = getDataSplit() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rarDamLU1SRs"
      },
      "source": [
        "##### Level 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49pmnA2rzg21"
      },
      "source": [
        "x_test=tokenizer.texts_to_sequences(x_test)\n",
        "xtest=pad_sequences(x_test, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "y_pred=model.predict(xtest,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "results = confusion_matrix(y_test, np.round(y_pred)) \n",
        "print('Confusion Matrix general:')\n",
        "print(results) \n",
        "print('Accuracy Score general:',accuracy_score(y_test, np.round(y_pred)))\n",
        "print('Report general: ')\n",
        "print(classification_report(y_test, np.round(y_pred),digits=4,target_names=['non-muslim','muslim']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTkxf0md3Ihn"
      },
      "source": [
        "##### Level 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7I8dPzu3KgY"
      },
      "source": [
        "xp_test=tokenizer.texts_to_sequences(xp_test)\n",
        "xp_test=pad_sequences(xp_test, maxlen=max_char,padding='post', truncating='post')\n",
        "pred_p=model.predict(xp_test,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "\n",
        "X= map_logreg_features(y_pred,pred_p,rfe.support_)\n",
        "\n",
        "predictions = logreg.predict(X)\n",
        "print(\"general predictions:\")\n",
        "results = confusion_matrix(y_test, np.round(predictions)) \n",
        "print(results) \n",
        "print('REDS TEST Accuracy Score Stage2:',accuracy_score(y_test, np.round(predictions)))\n",
        "print('Report general: ')\n",
        "print(classification_report(y_test, np.round(predictions), digits=4,target_names=['non-muslim','muslim']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxh1BIQbARmV"
      },
      "source": [
        "#### TEST SBM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXZe-JJ1R2EW"
      },
      "source": [
        "x_sbm,xp_sbm,y_sbm=load_upsbm()\n",
        "\n",
        "padded_sequences_xsbm=tokenizer.texts_to_sequences(x_sbm)\n",
        "xsbm=pad_sequences(padded_sequences_xsbm, maxlen=max_char,padding='post', truncating='post')\n",
        "y_pred_sbm=model.predict(xsbm,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "\n",
        "if concat_model==False:\n",
        "  padded_sequences_xpsbm=tokenizer.texts_to_sequences(xp_sbm)\n",
        "  xpsbm=pad_sequences(padded_sequences_xpsbm, maxlen=max_char,padding='post', truncating='post')\n",
        "  y_pred_sbm_p=model.predict(xpsbm,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnOl61FhL24e"
      },
      "source": [
        "##### Level 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1v6JIjusJqK"
      },
      "source": [
        "results = confusion_matrix(y_sbm==\"Muslim\", np.round(y_pred_sbm)) \n",
        "print('Confusion Matrix general:')\n",
        "print(results) \n",
        "print('Accuracy Score general:',accuracy_score(y_sbm==\"Muslim\", np.round(y_pred_sbm)))\n",
        "print('Report general: ')\n",
        "print(classification_report(y_sbm==\"Muslim\", np.round(y_pred_sbm),digits=4,target_names=['non-muslim','muslim']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ruJfbvwMCtN"
      },
      "source": [
        "##### Level 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuF6RNFnAoWU"
      },
      "source": [
        "X_sbm= map_logreg_features(y_pred_sbm,y_pred_sbm_p,rfe.support_)\n",
        "predictions = logreg.predict(X_sbm)\n",
        "\n",
        "print(\"general predictions:\")\n",
        "results = confusion_matrix(y_sbm==\"Muslim\", np.round(predictions)) \n",
        "# print('Confusion Matrix general:')\n",
        "print(results) \n",
        "print('Accuracy Score general:',accuracy_score(y_sbm==\"Muslim\", np.round(predictions)))\n",
        "print('Report general: ')\n",
        "print(classification_report(y_sbm==\"Muslim\", np.round(predictions), digits=4,target_names=['non-muslim','muslim']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPPL5XzkUHbR"
      },
      "source": [
        "# Explaining CNN Predictions using LRP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpSx2u8VUN8g"
      },
      "source": [
        "x_train,x_val,x_test,xp_train,xp_val,xp_test,xs_train,xs_val,xs_test,y_train,y_val,y_test = getDataSplit() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytPsQ9VT996m"
      },
      "source": [
        "with open(data_dir+\"tokenizer.pkl\", \"rb\") as f:\n",
        "    (tokenizer,max_char)=pickle.load(f)\n",
        "padded_sequences_test=tokenizer.texts_to_sequences(x_test)\n",
        "xtest=pad_sequences(padded_sequences_test, maxlen=max_char,padding='post', truncating='post')\n",
        "model=load_model(model_dir+'CNN.h5')\n",
        "pred=model.predict(xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SIaWfhGuSDA"
      },
      "source": [
        "from matplotlib import cm, transforms,colors\n",
        "def plot_text_heatmap(chars, scores, filename, title=\"\", width=10, height=0.2, verbose=0, max_word_per_line=46):\n",
        "    plt.clf() \n",
        "    fig = plt.figure(figsize=(width, height))\n",
        "    \n",
        "    ax = plt.gca()\n",
        "\n",
        "    ax.set_title(title, loc='left')\n",
        "    tokens = chars\n",
        "    # if verbose > 0:\n",
        "    #     print('len words : %d | len scores : %d' % (len(words), len(scores)))\n",
        "\n",
        "    cmap = plt.cm.ScalarMappable(cmap=cm.bwr)\n",
        "    cmap.set_clim(0, 1)\n",
        "    \n",
        "    canvas = ax.figure.canvas\n",
        "    t = ax.transData\n",
        "\n",
        "    # normalize scores to the followings:\n",
        "    # - negative scores in [0, 0.5]\n",
        "    # - positive scores in (0.5, 1]\n",
        "    normalized_scores = 0.5 * scores / np.max(np.abs(scores)) + 0.5\n",
        "    \n",
        "    if verbose > 1:\n",
        "        print('Raw score')\n",
        "        print(scores)\n",
        "        print('Normalized score')\n",
        "        print(normalized_scores)\n",
        "\n",
        "    # make sure the heatmap doesn't overlap with the title\n",
        "    loc_y = -0.2\n",
        "    threshold=normalized_scores.max()/4\n",
        "    textcolors= [\"black\", \"white\"]\n",
        "    for i, token in enumerate(tokens):\n",
        "        *rgb, _ = cmap.to_rgba(normalized_scores[i], bytes=True)\n",
        "        color = '#%02x%02x%02x' % tuple(rgb)\n",
        "        \n",
        "        text = ax.text(0.0, loc_y, token, color= textcolors[int(normalized_scores[i] < threshold)],\n",
        "                       bbox={\n",
        "                           'facecolor': color,\n",
        "                           'pad': 5.0,\n",
        "                           'linewidth': 1,\n",
        "                           'boxstyle': 'round,pad=0.5'\n",
        "                       }, transform=t)\n",
        "\n",
        "        text.draw(canvas.get_renderer())\n",
        "        ex = text.get_window_extent()\n",
        "        \n",
        "        # create a new line if the line exceeds the length\n",
        "        if (i+1) % max_word_per_line == 0:\n",
        "            loc_y = loc_y -  2.5\n",
        "            t = ax.transData\n",
        "        else:\n",
        "            t = transforms.offset_copy(text._transform, x=ex.width+15, units='dots')\n",
        "\n",
        "    if verbose == 0:\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.savefig(output_dir+filename+\".png\",dpi=90,format='png',bbox_inches='tight')#pad_inches=.5)\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlQxbv15EsT2"
      },
      "source": [
        "df=pd.DataFrame({\"y_true\":y_test,\"xname\":x_test,\"y_pred\":pred.flatten(),\"sex\":xs_test})\n",
        "df_correct_NM = df[(df.y_pred<0.5) & (df.y_true==0.0)]\n",
        "df_correct_M = df[(df.y_pred>=0.5) & (df.y_true==1.0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX7WJM7wpsS1"
      },
      "source": [
        "df_nm_female=df_correct_NM[df_correct_NM.sex]\n",
        "df_nm_female=df_nm_female.nsmallest(5, 'y_pred')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5a5hdOntngm"
      },
      "source": [
        "df_m_female=df_correct_M[df_correct_M.sex]\n",
        "dfplpl=dfplpl.nlargest(5, 'y_pred')\n",
        "dfplpl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aWUWNs0QVxs"
      },
      "source": [
        "df_NM=df_correct_NM.nsmallest(20, 'y_pred')\n",
        "df_M=df_correct_M.nlargest(50, 'y_pred')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ8hdTNqV5__"
      },
      "source": [
        "names=df_M.xname\n",
        "padded_sequences=tokenizer.texts_to_sequences(names)\n",
        "x=pad_sequences(padded_sequences, maxlen=max_char,padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNv1GTImuaRv"
      },
      "source": [
        "def explain(x):\n",
        "  with DeepExplain(session=K.get_session()) as de:\n",
        "          model=load_model(model_dir+'CNN.h5'})\n",
        "          dense = Dense(1,kernel_initializer=\"he_uniform\")(model.layers[-2].output)\n",
        "          fmodel=Model(model.input,dense)\n",
        "          fmodel.set_weights(model.get_weights())\n",
        "          fmodel.compile(optimizer='nadam', loss='binary_crossentropy',metrics=['accuracy',f1])\n",
        "          input_tensor = fmodel.get_layer(\"input_13\").input #layers[0].input\n",
        "          embedding = fmodel.get_layer(\"embedding_13\").output #layers[1].output\n",
        "          get_embedding_out = K.function([input_tensor],[embedding])\n",
        "          embedding_out=get_embedding_out([x])[0]\n",
        "          pre_softmax_tensor = fmodel.layers[-1].output\n",
        "          attributions = de.explain('elrp',  pre_softmax_tensor, embedding, embedding_out)\n",
        "          attr= np.sum(attributions, -1)\n",
        "    return attr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv4GscvnYO1S"
      },
      "source": [
        "#For global attributions on test set\n",
        "df=pd.DataFrame({\"name\":x_test,\"y\":y_test,\"pred\":pred.flatten(),\"sex\":xs_test})\n",
        "attr= explain(x_test)\n",
        "attrdf=pd.DataFrame(attr)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "attrdf.reset_index(drop=True, inplace=True)\n",
        "df2=pd.concat([df,attrdf],axis=1,sort=False)\n",
        "df2.to_csv(output_dir+\"attributions.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtLMeGC7AOLX"
      },
      "source": [
        "# Plot of decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID_uEzHzJUpr"
      },
      "source": [
        "# define function to map higher order polynomial features\n",
        "def mapFeature(X1,X2,support=None, degree=None,poly=False,gender=None): \n",
        "    if poly==True:\n",
        "      res = np.ones(X1.shape[0])\n",
        "      \n",
        "      for i in range(1,degree + 1):\n",
        "            for j in range(0,i + 1):\n",
        "                res = np.column_stack((res, (X1 ** (i-j)) * (X2 ** j)))\n",
        "    else:\n",
        "      if gender==1:\n",
        "        g=np.ones((len(X1)))# assumer all non female\n",
        "      else:\n",
        "        g=np.zeros((len(X1)))\n",
        "      res=map_logreg_features(X1,X2,support)\n",
        "    return res\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "def costFunc(theta, X, y):\n",
        "  m = y.shape[0]\n",
        "  z = X.dot(theta)\n",
        "  h = sigmoid(z)\n",
        "  term1 = y * np.log(h)\n",
        "  term2 = (1- y) * np.log(1 - h)\n",
        "  J = -np.sum(term1 + term2, axis = 0) / m\n",
        "  return J \n",
        "\n",
        "def plotDecisionBoundary(axes,degree=None,support=None,poly=False,gender=0,mod=None): \n",
        "  u = np.linspace(0, 1, 2000)\n",
        "  v = np.linspace(0, 1, 2000)\n",
        "  U,V = np.meshgrid(u,v)\n",
        "  # convert U, V to vectors for calculating additional features\n",
        "  # using vectorized implementation\n",
        "  U = np.ravel(U)\n",
        "  V = np.ravel(V)\n",
        "  Z = np.zeros((len(u) * len(v)))\n",
        "  \n",
        "  X_poly = mapFeature(U, V,support, degree,poly,gender)\n",
        "  #   # Z = X_poly.dot(mod)\n",
        "  Z = mod.predict(X_poly)\n",
        "  \n",
        "  # reshape U, V, Z back to matrix\n",
        "  U = U.reshape((len(u), len(v)))\n",
        "  V = V.reshape((len(u), len(v)))\n",
        "  Z = Z.reshape((len(u), len(v)))\n",
        "  \n",
        "  cs = axes.contour(U,V,Z,levels=[0], linewidths=2,cmap= \"Greys_r\")\n",
        "\n",
        "  # axes.legend(labels=['Muslim', 'Non-Muslim', 'Decision Boundary'])\n",
        "  return cs\n",
        "\n",
        "def plot_boundary(df,poly,gender=0):\n",
        "  pos = df['muslim'] == 1\n",
        "  neg = df['muslim'] == 0\n",
        "  if poly==True:\n",
        "    degree = 10\n",
        "\n",
        "# Get the features \n",
        "  X = df.iloc[:, :2]\n",
        "  if poly==True:\n",
        "    X_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1],None, degree,poly)\n",
        "  else:\n",
        "    X_poly= mapFeature(X.iloc[:, 0], X.iloc[:, 1], rfe.support_,None,poly,gender)\n",
        "  # Get the target variable\n",
        "  y = df.iloc[:, 2]\n",
        "\n",
        "  fig, axes = plt.subplots(figsize=(8, 8));\n",
        "  axes.set_frame_on(False)\n",
        "  \n",
        "  # axes.axvline()\n",
        "  # axes.axes.axhline()\n",
        "\n",
        "  axes.set_xlabel('Probability Individual')\n",
        "  axes.set_ylabel('Probability Parent/Spouse')\n",
        "  \n",
        "  axes.scatter(df.loc[pos, 'pred'], df.loc[pos, 'pred_p'], color = 'red', marker='+', label='Muslim',alpha=.8)\n",
        "  axes.scatter(df.loc[neg, 'pred'], df.loc[neg, 'pred_p'], color = 'blue', marker='.', label='Non-Muslim',alpha=0.7)\n",
        "  axes.grid(color=(.99,.99,.99), linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "  \n",
        "  axes.legend(bbox_to_anchor=(.62,-.1),framealpha=1,edgecolor='black',facecolor='white')\n",
        "  axes.set_xlim(0,1)\n",
        "  axes.set_ylim(0,1)\n",
        "  font = {'family' : 'normal',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 12}\n",
        "  plt.rc('font', **font)\n",
        "  if poly==True:\n",
        "    mod=LogisticRegression(verbose=1,class_weight='balanced')\n",
        "  else: \n",
        "    plotDecisionBoundary(axes,None,rfe.support_, poly,gender,logreg)\n",
        "\n",
        "  axes.set_rasterized(True)\n",
        "  plt.savefig(output_dir+\"decision_boundary.eps\",dpi=250,format='eps',bbox_inches='tight')#pad_inches=.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXgH9RetzQqy"
      },
      "source": [
        "_,_,x_test,_,_,xp_test,_,_,y_test = getDataSplit() \n",
        "x_test=tokenizer.texts_to_sequences(x_test)\n",
        "xtest=pad_sequences(x_test, maxlen=max_char,padding='post', truncating='post')\n",
        "pred=mod_append.predict(xtest, batch_size=1024,verbose=1, use_multiprocessing=True, workers=-1)\n",
        "\n",
        "xp_test=tokenizer.texts_to_sequences(xp_test)\n",
        "xptest=pad_sequences(xp_test, maxlen=max_char,padding='post', truncating='post')\n",
        "pred_p=mod_append.predict(xptest, batch_size=1024,verbose=1, use_multiprocessing=True, workers=-1)\n",
        "\n",
        "dfm=pd.DataFrame({'pred':pred.flatten(),'pred_p':pred_p.flatten(), 'muslim':y_test})\n",
        "\n",
        "plot_boundary(dfm,poly=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}