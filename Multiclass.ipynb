{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Name_to_religion_multiclass.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fMJrY0StzVtI",
        "atGqtJaEWpkm",
        "QJfZ2KUC71X4",
        "oGUD7YaTMsUd",
        "xBCuE_GcTNew",
        "z6N2Y86ZIpLT",
        "H-5x83oIaPlc"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RochanaChaturvedi/it-is-all-in-the-name/blob/add-license-1/Multiclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI6BGIJwj78n"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCey5dgV8IzL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXXVHkaNkbHR"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelBinarizer,LabelEncoder\n",
        "import pickle \n",
        "\n",
        "from collections import Counter\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold, KFold\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,initializers,regularizers,constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Multiply,Add,Permute,Dot,Activation,\\\n",
        "    AlphaDropout,RepeatVector, Dense, Bidirectional,LSTM, BatchNormalization, LSTM,SpatialDropout1D,Dropout,\\\n",
        "    GlobalMaxPool1D,GlobalAveragePooling1D,Concatenate,Input,Conv1D, MaxPool1D, Embedding,TimeDistributed,Lambda, Reshape,TimeDistributed\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import tensorflow.keras\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.feature_selection import RFE\n",
        "from itertools import compress\n",
        "from sklearn.metrics import recall_score,log_loss\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import seaborn as sns\n",
        "\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88-kNEn4Ho4s"
      },
      "source": [
        "# Global constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WadODWVVHrzK"
      },
      "source": [
        "eps=1e-12\n",
        "np.random.seed(42)\n",
        "\n",
        "base_dir= \"/content/drive/My Drive/name_to_religion/\"\n",
        "if not os.path.isdir(base_dir):\n",
        "    os.mkdir(base_dir)\n",
        "\n",
        "data_dir= base_dir + \"data/\"\n",
        "if not os.path.isdir(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "model_dir= base_dir+ \"model/\"\n",
        "if not os.path.isdir(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQo1p2uELQS6"
      },
      "source": [
        "# Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qri9bhM4kZ_H"
      },
      "source": [
        "# Run Once to save split (for pair programming)\n",
        "def getTrain_data():\n",
        "  data=pd.read_csv(data_dir+\"REDS_cleaned.csv\")\n",
        "  x=data.name\n",
        "  xp=data.pname\n",
        "  xi=data.index\n",
        "  xs=data.sex\n",
        "  Y=data.religion\n",
        "  (x_train1, x_test,xp_train1,xp_test, Y_train1,Y_test,xi_train1,xi_test) = train_test_split(x, xp,Y,xi, stratify=Y, random_state=42,test_size=0.15)\n",
        "  (x_train, x_val,xp_train,xp_val, Y_train,Y_val,xi_train,xi_val)= train_test_split(x_train1, xp_train1, Y_train1,xi_train1,random_state=42, stratify=Y_train1, test_size=0.17647)\n",
        "  df=pd.DataFrame({\"index\":xi_train,\"name\":x_train,\"parent\":xp_train,\"religion\":Y_train})\n",
        "  df.to_csv(data_dir+'REDS_train.csv')\n",
        "  df=pd.DataFrame({\"index\":xi_val,\"name\":x_val,\"parent\":xp_val,\"religion\":Y_val})\n",
        "  df.to_csv(data_dir+'REDS_val.csv')\n",
        "  df=pd.DataFrame({\"index\":xi_test,\"name\":x_test,\"parent\":xp_test,\"religion\":Y_test,\"muslim\":y_test})\n",
        "  df.to_csv(data_dir+'REDS_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wafTfgZ37gE_"
      },
      "source": [
        "# Load Data and Pre-process\n",
        "def load_reds_split():\n",
        "    data_train=pd.read_csv(data_dir+\"REDS_train.csv\",index_col=\"index\")\n",
        "    data_val=pd.read_csv(data_dir+\"REDS_val.csv\",index_col=\"index\")\n",
        "    data_test=pd.read_csv(data_dir+\"REDS_test.csv\",index_col=\"index\")\n",
        "    for data in [data_train,data_val,data_test]:\n",
        "      data.name.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "      data.parent.replace(np.nan, '', regex=True,inplace=True)\n",
        "\n",
        "      data.parent.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "      data.name=\"{\"+data.name.astype(str)+\"}\"\n",
        "      data.parent=\"{\"+data.parent.astype(str)+\"}\"\n",
        "      if(concat_model):\n",
        "        data.name='#'+data.name.astype(str)+'#'+data.parent.astype(str)+'#' \n",
        "\n",
        "    if non_neural==True:\n",
        "      data_train[\"religion_id\"]= data_train.religion.factorize()[0]\n",
        "      category_id_df = data_train[['religion', 'religion_id']].drop_duplicates().sort_values('religion_id')\n",
        "      category_to_id = dict(category_id_df.values)\n",
        "      id_to_category = dict(category_id_df[['religion_id', 'religion']].values)\n",
        "      Y_train=data_train[\"religion\"].map(category_to_id)\n",
        "      Y_val=data_val[\"religion\"].map(category_to_id)\n",
        "      Y_test=data_test[\"religion\"].map(category_to_id)\n",
        "      return data_train.name,data_val.name,data_test.name,data_train.parent,data_val.parent,data_test.parent,Y_train,Y_val,Y_test,category_to_id,id_to_category\n",
        "    else:\n",
        "      lbe = LabelBinarizer()\n",
        "      lbe.fit(data_train.religion)\n",
        "      return data_train.name,data_val.name,data_test.name,data_train.parent,data_val.parent,data_test.parent,data_train.religion,data_val.religion,data_test.religion,lbe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruezfq472I9u"
      },
      "source": [
        "#Load Secondary dataset and pre-process\n",
        "def load_upsbm():\n",
        "  data_test_sbm = pd.read_csv(data_dir+\"upsbm_annotated.csv\")\n",
        "\n",
        "  data_test_sbm.familyhead=data_test_sbm.familyhead.str.upper()\n",
        "  data_test_sbm.fatherhusbandname=data_test_sbm.fatherhusbandname.str.upper()\n",
        "\n",
        "  data_test_sbm.familyhead.replace(\"\\.\",\" \", regex=True,inplace=True)\n",
        "  data_test_sbm.familyhead.replace(\"([A-Z])\\\\1\\\\1+\",\"\\\\1\", regex=True,inplace=True)\n",
        "  data_test_sbm.familyhead.replace(\"\\s+\",\" \", regex=True,inplace=True)\n",
        "\n",
        "  data_test_sbm.fatherhusbandname.replace(\"\\.\",\" \", regex=True,inplace=True)\n",
        "  data_test_sbm.fatherhusbandname.replace(\"([A-Z])\\\\1\\\\1+\",\"\\\\1\", regex=True,inplace=True)\n",
        "  data_test_sbm.fatherhusbandname.replace(\"\\s+\",\" \", regex=True,inplace=True)\n",
        "\n",
        "  data_test_sbm = data_test_sbm.apply(lambda x: x.str.strip(\" \") if x.dtype == \"object\" else x)\n",
        "  # data_test_sbm.to_csv(\"/content/drive/My Drive/name_to_religion/data/upsbm_annotated2.csv\")\n",
        "  \n",
        "  data_test_sbm.familyhead.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "  data_test_sbm.fatherhusbandname.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "\n",
        "  y_sbm = data_test_sbm.muslim.map(lambda y:\"Muslim\" if y==1 else \"Others\")\n",
        "  x_sbm=\"{\"+data_test_sbm.familyhead.astype(str)+\"}\"\n",
        "  xp_sbm=\"{\"+data_test_sbm.fatherhusbandname.astype(str)+\"}\"\n",
        "  if concat_model==True:\n",
        "    x_sbm=\"#\"+x_sbm+\"#\"+xp_sbm+\"#\"\n",
        "  return x_sbm,xp_sbm,y_sbm\n",
        "\n",
        "# x_sbm,xp_sbm,y_sbm=load_upsbm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsr2yN6vLoeU"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjybXNsqdmhM"
      },
      "source": [
        "## Non-neural: SVM Linear/Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tq5wI5TRxI2"
      },
      "source": [
        "concat_model=True\n",
        "non_neural=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h89aJ8GvCfA"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMJrY0StzVtI"
      },
      "source": [
        "### fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmVc2F86FRg3"
      },
      "source": [
        "loss_max=100\n",
        "def objective(space):\n",
        "  max_ngram=int(space['max_ngram'])\n",
        "  iter=int(space['iter'])\n",
        "  vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1,max_ngram))#12))#max_features=40000\n",
        "  vectorizer.fit(x_train)\n",
        "  tfidf_matrix_train = vectorizer.transform(x_train)\n",
        "  tfidf_matrix_val = vectorizer.transform(x_val)\n",
        "\n",
        "  if classifier==\"LOGIT\":\n",
        "    clf = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=-1, C=space['C'],max_iter=iter)# penalty='l2',max_iter=500,multi_class='multinomial')\n",
        "  else:\n",
        "    clf = svm.LinearSVC(class_weight='balanced', C= space['C'],max_iter=iter)\n",
        "    clf = CalibratedClassifierCV(clf) \n",
        " \n",
        "  clf.fit(tfidf_matrix_train, Y_train)\n",
        "    \n",
        "  # Predict on Cross Validation data\n",
        "  pred = clf.predict_proba(tfidf_matrix_val)\n",
        "  global loss_max\n",
        "  loss = log_loss(Y_val, pred)\n",
        "  out_file = model_dir+'multiclass_'+classifier+'concat'+str(concat_model)+'trials.csv'\n",
        "  of_connection = open(out_file, 'w')\n",
        "  writer = csv.writer(of_connection)\n",
        "\n",
        "  if(loss<=loss_max):\n",
        "    filename=model_dir+\"vectorizer\"+str(max_ngram)+\".sav\"\n",
        "    pickle.dump(vectorizer,open(filename, 'wb'))\n",
        "    filename = model_dir+ 'multiclass_'+classifier+'concat'+str(concat_model)+'.sav'\n",
        "    pickle.dump(clf, open(filename, 'wb'))\n",
        "    loss_g=loss\n",
        "    print(classifier+\" \"+str(concat_model)+\"loss:\"+str(loss)+\"C:\"+str(space['C'])+\"ngram:\"+str(max_ngram))\n",
        "  return {'loss': loss , 'status': STATUS_OK}\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd8EziRd9fSo"
      },
      "source": [
        "for classifier in [\"LOGIT\",\"SVM\"]:\n",
        "  for concat_model in [True,False]:\n",
        "    gc.collect()\n",
        "    x_train,x_val, x_test, xp_train,xp_val, xp_test,Y_train,Y_val, Y_test,category_to_id,id_to_category = load_reds_split() \n",
        "    if concat_model==False:\n",
        "        data1=pd.DataFrame({\"name\":x_train,\"religion\":Y_train})\n",
        "        data2=pd.DataFrame({\"name\":xp_train,\"religion\":Y_train})#.apply(lambda x: x.split(\" \")[0])})\n",
        "        data2=data2[data2[\"name\"].notna()]\n",
        "        data2=data2[data2.name!=\"\"]\n",
        "        d=pd.concat([data1,data2],axis=0)\n",
        "        x_train=d.name\n",
        "        Y_train=d.religion\n",
        "\n",
        "    space={'C':hp.uniform('C',0,100),'max_ngram':hp.quniform('x_max_ngram',1,12,1),'iter':hp.quniform('iter',60,120,1)}\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=objective,\n",
        "                space=space,\n",
        "                algo=tpe.suggest,\n",
        "                max_evals=10,\n",
        "                trials=trials)\n",
        "    \n",
        "    print(classifier,concat_model, best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atGqtJaEWpkm"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DddOLhZ4f3vf"
      },
      "source": [
        "non_neural=True\n",
        "for classifier in [\"SVM\"]:\n",
        "  for concat_model in [False]:\n",
        "    if classifier==\"LOGIT\":\n",
        "      if concat_model:\n",
        "        opt_C=33.38829440124558\n",
        "        max_iter=105\n",
        "        max_ngram=10\n",
        "      else:\n",
        "        opt_C= 64.48692955599071\n",
        "        max_iter= 114\n",
        "        max_ngram= 12\n",
        "    else:\n",
        "      if concat_model:\n",
        "        opt_C=8.472897550492439\n",
        "        max_iter=1000\n",
        "        max_ngram=10\n",
        "      else:\n",
        "        opt_C=79.52865215798708\n",
        "        max_iter=1000\n",
        "        max_ngram=11\n",
        "    print(classifier,concat_model)\n",
        "    gc.collect()\n",
        "    x_train,x_val, x_test, xp_train,xp_val, xp_test,Y_train,Y_val, Y_test,category_to_id,id_to_category = load_reds_split() \n",
        "\n",
        "    if concat_model==False:\n",
        "        data1=pd.DataFrame({\"name\":x_train,\"religion\":Y_train})\n",
        "        data2=pd.DataFrame({\"name\":xp_train,\"religion\":Y_train})\n",
        "        data2=data2[data2[\"name\"].notna()]\n",
        "        data2=data2[data2.name!=\"\"]\n",
        "        d=pd.concat([data1,data2],axis=0)\n",
        "        x_train=d.name\n",
        "        Y_train=d.religion\n",
        "\n",
        "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1,max_ngram))\n",
        "    vectorizer.fit(x_train)\n",
        "        \n",
        "    tfidf_matrix_train = vectorizer.transform(x_train)\n",
        "    tfidf_matrix_val = vectorizer.transform(x_val)\n",
        "    tfidf_matrix_test = vectorizer.transform(x_test)\n",
        "\n",
        "    if classifier==\"LOGIT\":\n",
        "        clf = LogisticRegression(verbose=2,class_weight='balanced', random_state=42, n_jobs=-1, C=opt_C,max_iter=max_iter)\n",
        "    else:\n",
        "        clf = svm.LinearSVC(verbose=2,class_weight='balanced', C= opt_C, max_iter=max_iter,random_state=42)\n",
        "    clf.fit(tfidf_matrix_train, Y_train)\n",
        "\n",
        "    pickle.dump(clf,open(model_dir+'model_multiclass_'+classifier+'_concat_'+str(concat_model)+'.sav',\"wb\"))\n",
        "    pickle.dump(vectorizer,open(model_dir+'vectorizer_multiclass_'+classifier+'_concat_'+str(concat_model)+'.sav',\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMLZzoJjWsO1"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYGZ1njrR5xJ"
      },
      "source": [
        "non_neural=True\n",
        "gc.collect()\n",
        "x_train,x_val, x_test, xp_train,xp_val, xp_test,Y_train,Y_val, Y_test,category_to_id,id_to_category = load_reds_split() \n",
        "x_sbm,xp_sbm,y_sbm=load_upsbm()\n",
        "\n",
        "for classifier in [\"LOGIT\",\"SVM\"]:\n",
        "  for concat_model in [True,False]:\n",
        "    print(classifier,concat_model)\n",
        "\n",
        "    vectorizer = pickle.load(open(model_dir+'vectorizer_multiclass_'+classifier+'_concat_'+str(concat_model)+'.sav','rb'))\n",
        "    clf = pickle.load(open(model_dir+'model_multiclass_'+classifier+'_concat_'+str(concat_model)+'.sav','rb'))\n",
        "  \n",
        "    tfidf_matrix_val = vectorizer.transform(x_val)\n",
        "    tfidf_matrix_test = vectorizer.transform(x_test)\n",
        "    tfidf_matrix_sbm= vectorizer.transform(x_sbm)\n",
        "\n",
        "#reds val\n",
        "    y_pred_val = clf.predict(tfidf_matrix_val)\n",
        "    df=pd.DataFrame({\"familyhead\":x_val,\"parent\":xp_val,\"religion\":Y_val,\n",
        "                 \"predicted\":pd.Series(y_pred_val).map(id_to_category)})\n",
        "    # df.to_csv(data_dir+'val_predictions_multiclass_'+classifier+'_concat_'+str(concat_model)+'.csv')\n",
        "\n",
        "    tagset=id_to_category.values()\n",
        "    print(classifier)\n",
        "    print(concat_model)\n",
        "    print('REDS IN SAMPLE Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(Y_val,y_pred_val)))\n",
        "    print(classification_report(Y_val, y_pred_val,digits=4,target_names=tagset))\n",
        "    conf_mat = confusion_matrix(Y_val,y_pred_val)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "                xticklabels=category_to_id.keys(), yticklabels=category_to_id.keys())\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "#reds_test\n",
        "    y_pred_test = clf.predict(tfidf_matrix_test)\n",
        "    df = pd.DataFrame({\"familyhead\":x_test,\"parent\":xp_test,\"religion\":Y_test,\n",
        "                 \"predicted\":pd.Series(y_pred_test).map(id_to_category)})\n",
        "    # df.to_csv(data_dir+'test_predictions_multiclass_'+classifier+'_concat_'+str(concat_model)+'.csv')\n",
        "\n",
        "    print('REDS out of SAMPLE Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(Y_test,y_pred_test)))\n",
        "    print(classification_report(Y_test, y_pred_test,digits=4,target_names=tagset))\n",
        "    conf_mat = confusion_matrix(Y_test,y_pred_test)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "                xticklabels=category_to_id.keys(), yticklabels=category_to_id.keys())\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "#UPSBM    \n",
        "    y_pred_sbm=clf.predict(tfidf_matrix_sbm)\n",
        "    df=pd.DataFrame({\"familyhead\":x_sbm,\"parent\":xp_sbm,\"religion\":y_sbm,\n",
        "                 \"predicted\":pd.Series(y_pred_sbm).map(id_to_category)})\n",
        "\n",
        "    # df.to_csv(data_dir+'sbm_predictions_multiclass_'+classifier+'_concat_'+str(concat_model)+'.csv')\n",
        "\n",
        "    print('SBM Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(y_sbm==\"Muslim\",y_pred_sbm)))\n",
        "    print(classification_report(y_sbm==\"Muslim\",y_pred_sbm,digits=4))\n",
        "    conf_mat = confusion_matrix(y_sbm==\"Muslim\",y_pred_sbm)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "                xticklabels={\"nonMuslim\",\"Muslim\"}, yticklabels={\"nonMuslim\",\"Muslim\"})\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E060TugtSCNN"
      },
      "source": [
        "# Neural Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EI0_Ayo8MUt"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnbLpmPiq775"
      },
      "source": [
        "def tokenize(x,max_char):\n",
        "  unique_symbols = Counter()\n",
        "\n",
        "  for _, name in x.iteritems():\n",
        "      unique_symbols.update(name)\n",
        "\n",
        "  num_unique_symbols = len(unique_symbols) #- len(uncommon_symbols) + 1 \n",
        "  print(\"Unique symbols:\",num_unique_symbols, unique_symbols)\n",
        "\n",
        "  tokenizer = Tokenizer(\n",
        "      char_level=True,\n",
        "      filters=None,\n",
        "      oov_token='*',\n",
        "      lower=False,\n",
        "      num_words=None\n",
        "  )\n",
        "\n",
        "  tokenizer.fit_on_texts([x])\n",
        "  char_dict = {}\n",
        "  ind_to_char={}\n",
        "  for i, char in enumerate(unique_symbols):\n",
        "    char_dict[char] = i + 1\n",
        "    ind_to_char[i+1]=char\n",
        "  tokenizer.word_index = char_dict.copy()\n",
        "  print(char_dict)\n",
        "  \n",
        "  tokenizer.word_index[tokenizer.oov_token] = max(char_dict.values()) + 1\n",
        "  ind_to_char[max(char_dict.values()) + 1]=tokenizer.oov_token\n",
        "  max_char=max(map(len,x))\n",
        "  x_sequences = tokenizer.texts_to_sequences(x)\n",
        "  with open(data_dir+\"tokenizer.pkl\", \"wb\") as f:\n",
        "      pickle.dump((tokenizer,max_char), f)\n",
        "  \n",
        "  return (num_unique_symbols+1,tokenizer)\n",
        "\n",
        "\n",
        "def getMaxchar():\n",
        "  data=pd.read_csv(data_dir+\"REDS_cleaned.csv\")\n",
        "\n",
        "  data=  data[data[\"name\"].notna()]\n",
        "  data.pname = data.pname.replace(np.nan, '', regex=True)\n",
        "  data=data[data[\"religion\"].notna()]\n",
        "  data=data[~data.religion.isin([\"Others\"])]#,\"Jain\",\"Buddhist\"])]\n",
        "  data.drop_duplicates(subset =[\"name\",\"pname\",\"sex\",\"religion\"],keep = \"first\", inplace = True)\n",
        "  data.name.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "  data.pname.replace(\" \",\"}{\", regex=True,inplace=True)\n",
        "  data.name=\"{\"+data.name.astype(str)+\"}\"\n",
        "  data.pname=\"{\"+data.pname.astype(str)+\"}\"\n",
        "  if(concat_model):\n",
        "    data.name=\"#\"+data.name.astype(str)+'#'+data.pname.astype(str)+\"#\"\n",
        "    # data.pname=\"#\"+data.pname.astype(str)+\"#\"\n",
        "  max_char=max(map(len,data.name))\n",
        "  vocab_size,tokenizer=tokenize(data.name,max_char)\n",
        "  return max_char,tokenizer,vocab_size\n",
        "\n",
        "max_char,tokenizer,vocab_size=getMaxchar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7q7N4G_o1m2"
      },
      "source": [
        "def plot_hist(history):\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.plot(history['loss'], label='train_loss')\n",
        "  plt.plot(history['val_loss'], label='test_loss')\n",
        "  plt.title('Training and validation Loss')\n",
        "  plt.legend()\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.plot(history['accuracy'], label='train_Accuracy')\n",
        "  plt.plot(history['val_accuracy'], label='test_Accuracy')\n",
        "  plt.title('Training and validation Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJfZ2KUC71X4"
      },
      "source": [
        "### CNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saIuJYsK770t"
      },
      "source": [
        "def CNNmodel(shape, embedding_dim=15, activation='tanh', drop_embed=0.1, dropout=.25, dense_units=200,dense_activation='relu',\n",
        "                       kernel_sizes=[[1,50], [2, 100], [3,150], [5,200], [6,250]], initializer='glorot_uniform',\n",
        "                       optimizer='Nadam', batch_norm=True):\n",
        "  x_can = Input(shape=(shape[0],), dtype='int32')\n",
        "  \n",
        "  embedding = Embedding(shape[1], embedding_dim, input_length = shape[0], trainable = True, embeddings_initializer = initializer)(x_can)\n",
        "  embedding = BatchNormalization()(embedding)\n",
        "  embedding = Dropout(drop_embed)(embedding)\n",
        "  embedding = SpatialDropout1D(drop_embed)(embedding)\n",
        "  convs = []\n",
        "  \n",
        "  for [kernel_size, num_filter] in kernel_sizes:\n",
        "    l_conv = Conv1D(filters = num_filter, kernel_initializer = initializer, kernel_size = kernel_size,\n",
        "                    padding = 'valid', name = \"Convolution\"+str(kernel_size), activation = activation, strides = 1)(embedding)\n",
        "    l_pool= GlobalMaxPool1D()(l_conv)\n",
        "    l_pool=AlphaDropout(.05)(l_pool)\n",
        "    convs.append(l_pool)\n",
        "\n",
        "  x = Concatenate(axis=1)(convs)\n",
        "  x = Dense(dense_units, kernel_initializer=initializer, activation=dense_activation)(x)\n",
        "  x = Dropout(dropout)(x)  \n",
        "  x = Dense(6, kernel_initializer=initializer)(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(x_can,x)\n",
        "  model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])#\n",
        "  # print(model.summary())\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGUD7YaTMsUd"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBCuE_GcTNew"
      },
      "source": [
        "#### keras tuner: First hyperparameter tuner option"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYqTLOMOJ6Of"
      },
      "source": [
        "!pip install -U keras-tuner\n",
        "from kerastuner import HyperModel\n",
        "from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrg2iIeJKG2o"
      },
      "source": [
        "class MyHyperModel(HyperModel):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        \n",
        "        min_lr = hp.Float(name='min_lr',min_value=1e-5, max_value=1e-3, sampling='log')\n",
        "        factor = hp.Float(name='factor',min_value=.5, max_value=.8, sampling='linear')\n",
        "\n",
        "        epoch = hp.Int(name='epoch',min=20,max=80, sampling='linear')\n",
        "        batch_size = Integer(low=256, high=512, name='batch_size', prior='log-uniform',)\n",
        "        activation = Categorical(categories=['relu', 'tanh','elu'],\n",
        "                                     name='activation')\n",
        "        initializer = Categorical(categories=['he_uniform','glorot_uniform','he_normal'],\n",
        "                                     name='initializer')\n",
        "        batch_norm = Categorical(categories=[True,False],\n",
        "                                     name='batch_norm')\n",
        "        patience = Integer(low=3, high=5, name='patience')\n",
        "        dropout = hp.Float(name=\"dropout\",min_value=0,max_value=.5, sampling='linear')\n",
        "        emb_dropout = hp.Float(name=\"emb_dropout\",min_value=0,max_value=.25,sampling='linear')\n",
        "        dense_units= hp.Int(name='units',min_value=0, max_value=400, sampling='linear')\n",
        "        dense_act = hp.Choice(name='dense_act',values=['relu','tanh','sigmoid'])\n",
        "        filter=[]\n",
        "        for i in range(1,7):\n",
        "          filter.append(hp.Int( name=\"filter\"+str(i),min_value=0, max_value=300,sampling=\"linear\"))\n",
        "\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=factor,patience=3, min_lr=min_lr)\n",
        "        model=CNNmodel(shape,vocab_size, activation='elu', drop_embed=emb_dropout, dropout=dropout, dense_units=dense_units,dense_activation=dense_act,\n",
        "              kernel_sizes=[[1,filter[0]],[2,filter[1]], [3, filter[2]], [4,filter[3]], [5,filter[4]], [6,filter[5]]],#,[7,filter7]],#[8,filter8]],#,[9,filter9]],\n",
        "              initializer='glorot_uniform',optimizer='Nadam', batch_norm=True)        \n",
        "        return model\n",
        "\n",
        "hypermodel = MyHyperModel(num_classes=1)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOdfVB4LPUsd"
      },
      "source": [
        "non_neural=False\n",
        "x_train,x_val,x_test,xp_train,xp_val,xp_test,Y_train,Y_val,Y_test,encoder = load_reds_split()\n",
        "for concat_model in [True,False]:\n",
        "  if concat_model==False:\n",
        "    df1=pd.DataFrame({\"x\":x_train,\"y\":Y_train})\n",
        "    df2=pd.DataFrame({\"x\":xp_train,\"y\":Y_train})\n",
        "    df2 = df2[df2.x!=\"\"]\n",
        "    df=pd.concat([df1,df2],axis=0)\n",
        "\n",
        "    dfv1=pd.DataFrame({\"x\":x_val,\"y\":Y_val})\n",
        "    dfv2=pd.DataFrame({\"x\":xp_val,\"y\":Y_val})\n",
        "    dfv2=dfv2[dfv2[\"x\"].notna()]\n",
        "    dfv2 = dfv2[dfv2.x!=\"\"]\n",
        "    dfv=pd.concat([dfv1,dfv2],axis=0)\n",
        "    x_train=df.x\n",
        "    x_val=dfv.x\n",
        "    Y_train=df.y\n",
        "    Y_val=dfv.y\n",
        "  Y_test = encoder.transform(Y_test)\n",
        "  Y_val = encoder.transform(Y_val)\n",
        "  Y_train = encoder.transform(Y_train)\n",
        "  y_train_labels = np.argmax(Y_train, axis =1)\n",
        "  class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train_labels), y_train_labels)\n",
        "\n",
        "  padded_sequences_x=tokenizer.texts_to_sequences(x_train)\n",
        "  xtrain=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "  padded_sequences_x=tokenizer.texts_to_sequences(x_val)\n",
        "  xval=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "  padded_sequences_x=tokenizer.texts_to_sequences(x_test)\n",
        "  xtest=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "  class_weights = {i : class_weights[i] for i in range(6)}\n",
        "  shape=(xtrain.shape[1], vocab_size)\n",
        "  tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_loss',\n",
        "    max_epochs=25,\n",
        "    seed=10,\n",
        "    project_name='multi_religion')\n",
        "\n",
        "\n",
        "  # Search for the best parameters of the neural network using the contructed Hypberband tuner\n",
        "  tuner.search(xtrain, Y_train,\n",
        "              epochs=10, \n",
        "              validation_data=(xval, Y_val) )\n",
        "\n",
        "  # Get the best hyperparameters from the search\n",
        "  params = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "  # Build the model using the best hyperparameters\n",
        "  model = tuner.hypermodel.build(params)\n",
        "  model_name = model_dir+'multiclass_cnn_'+str(concat_model)+'.h5'\n",
        "  mc = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
        "\n",
        "  # Train the best fitting model\n",
        "  model.fit(xtrain, Y_train,validation_data=(xval, Y_val),initial_epoch =0, epochs = 80,\n",
        "                        shuffle=True, batch_size = 512,verbose=0,callbacks=[reduce_lr,mc],class_weight=class_weights,\n",
        "                        workers=-1, use_multiprocessing=True)\n",
        "\n",
        "  # Check the accuracy plots\n",
        "  hyperband_accuracy_df = pd.DataFrame(model.history.history)\n",
        "\n",
        "  hyperband_accuracy_df[['loss', 'accuracy']].plot()\n",
        "  plt.title('Loss & Accuracy Per EPOCH')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Accruacy')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6N2Y86ZIpLT"
      },
      "source": [
        "#### skopt: Second hyperparameter tuner option"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs1_tUoZIq8d"
      },
      "source": [
        "!pip install scikit-optimize \n",
        "import skopt\n",
        "from skopt import gbrt_minimize, gp_minimize\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.space import Real, Categorical, Integer \n",
        "from tensorflow.python.framework import ops\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.plots import plot_convergence\n",
        "from skopt.plots import plot_objective, plot_evaluations\n",
        "from skopt.utils import use_named_args\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFBRMizcIsK8"
      },
      "source": [
        "# for concat_model in [True,False]:\n",
        "min_lr = Real(low=1e-5, high=1e-3, prior='log-uniform',\n",
        "                        name='min_lr')\n",
        "factor = Real(low=.5, high=.8, prior='uniform',\n",
        "                        name='factor')\n",
        "\n",
        "epoch = Integer(low=20, high=80, name='epoch',prior='uniform')\n",
        "batch_size = Integer(low=256, high=512, name='batch_size', prior='log-uniform',)\n",
        "activation = Categorical(categories=['relu', 'tanh','elu'],\n",
        "                             name='activation')\n",
        "initializer = Categorical(categories=['he_uniform','glorot_uniform','he_normal'],\n",
        "                             name='initializer')\n",
        "batch_norm = Categorical(categories=[True,False],\n",
        "                             name='batch_norm')\n",
        "patience = Integer(low=3, high=5, name='patience')\n",
        "dropout = Real(low=0,high=.5,name=\"dropout\", prior='uniform')\n",
        "emb_dropout = Real(low=0,high=.25,name=\"emb_dropout\",prior='uniform')\n",
        "dense_units= Integer(low=0, high=400, name=\"dense_units\", prior='uniform')\n",
        "dense_act = Categorical(categories=['relu','tanh','sigmoid'],name='dense_act')\n",
        "\n",
        "dimensions = [min_lr,\n",
        "              factor,\n",
        "              epoch,\n",
        "              batch_size,\n",
        "              activation,\n",
        "              initializer,\n",
        "              dropout,\n",
        "              emb_dropout,\n",
        "              # batch_norm\n",
        "              dense_units,\n",
        "              dense_act\n",
        "            ]\n",
        "for i in range(1,7):\n",
        "  dimensions.append(Integer(low=0, high=300, name=\"filter\"+str(i)))\n",
        "\n",
        "# print(dimensions)\n",
        "default_parameters = [0.0003918194347141745,  0.7339073000818309, 56, 0.22291637642679563, 0.024993728954500728, 184, 'sigmoid', 43, 195, 17, 217, 282, 0]\n",
        "best_loss=100\n",
        "@use_named_args(dimensions=dimensions)\n",
        "def fitness(min_lr,  factor, epoch, dropout, emb_dropout,dense_units,dense_act,filter1,filter2,filter3,filter4,filter5,filter6):#,filter7):\n",
        "    \n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=factor,patience=3, min_lr=min_lr)\n",
        "    model=CNNmodel(shape,vocab_size, activation='elu', drop_embed=emb_dropout, dropout=dropout, dense_units=dense_units,dense_activation=dense_act,\n",
        "          kernel_sizes=[[1,filter1], [2, filter2], [3,filter3], [4,filter4], [5,filter5]],#,[7,filter7]],#[8,filter8]],#,[9,filter9]],\n",
        "          initializer='glorot_uniform',optimizer='Nadam', batch_norm=True)        \n",
        "    mc = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
        "\n",
        "    #named blackbox becuase it represents the structure\n",
        "    blackbox = model.fit(xtrain, Y_train,validation_data=(xval, Y_val),initial_epoch =0, epochs = epoch,\n",
        "                      shuffle=True, batch_size = 512,verbose=0,callbacks=[reduce_lr],class_weight=class_weights,\n",
        "                      workers=-1, use_multiprocessing=True)\n",
        "    \n",
        "    #return the validation accuracy for the last epoch.\n",
        "    loss = blackbox.history['val_loss'][-1]\n",
        "\n",
        "    # Print the classification accuracy.\n",
        "    print(\"loss: {0:.2%}\".format(loss))\n",
        "\n",
        "    # Save the model if it improves on the best-found performance.\n",
        "\n",
        "    # We use the global keyword so we update the variable outside of this function.\n",
        "    global best_loss\n",
        "    if loss < best_loss:\n",
        "        model.save(model_name)   \n",
        "        best_loss = loss\n",
        "    del model\n",
        "    \n",
        "    # Clear the Keras session, otherwise it will keep adding new models to the same TensorFlow graph each time we create\n",
        "    # a model with a different set of hyper-parameters.\n",
        "    K.clear_session()\n",
        "    ops.reset_default_graph()\n",
        "    print(min_lr,  factor, epoch, dropout, emb_dropout,dense_units,dense_act,filter1,filter2,filter3,filter4,filter5,filter6)\n",
        "   \n",
        "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtDzqpKh3fcH"
      },
      "source": [
        "for concat_model in [True, False]:\n",
        "  non_neural=False\n",
        "  x_train,x_val,x_test,xp_train,xp_val,xp_test,Y_train,Y_val,Y_test,encoder = load_reds_split()\n",
        "\n",
        "  if concat_model==False:\n",
        "    df1=pd.DataFrame({\"x\":x_train,\"y\":Y_train})\n",
        "    df2=pd.DataFrame({\"x\":xp_train,\"y\":Y_train})\n",
        "    df2 = df2[df2.x!=\"\"]\n",
        "    df=pd.concat([df1,df2],axis=0)\n",
        "\n",
        "    dfv1=pd.DataFrame({\"x\":x_val,\"y\":Y_val})\n",
        "    dfv2=pd.DataFrame({\"x\":xp_val,\"y\":Y_val})\n",
        "    dfv2=dfv2[dfv2[\"x\"].notna()]\n",
        "    dfv2 = dfv2[dfv2.x!=\"\"]\n",
        "    dfv=pd.concat([dfv1,dfv2],axis=0)\n",
        "    x_train=df.x\n",
        "    x_val=dfv.x\n",
        "    Y_train=df.y\n",
        "    Y_val=dfv.y\n",
        "  Y_test = encoder.transform(Y_test)\n",
        "  Y_val = encoder.transform(Y_val)\n",
        "  Y_train = encoder.transform(Y_train)\n",
        "  y_train_labels = np.argmax(Y_train, axis =1)\n",
        "  class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train_labels), y_train_labels)\n",
        "\n",
        "  padded_sequences_x=tokenizer.texts_to_sequences(x_train)\n",
        "  xtrain=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "  padded_sequences_x=tokenizer.texts_to_sequences(x_val)\n",
        "  xval=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "  padded_sequences_x=tokenizer.texts_to_sequences(x_test)\n",
        "  xtest=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "  class_weights = {i : class_weights[i] for i in range(6)}\n",
        "  shape=(xtrain.shape[1], vocab_size)\n",
        "\n",
        "  gp_result = gp_minimize(func = fitness,\n",
        "                              dimensions=dimensions,\n",
        "                              n_calls=40,\n",
        "                              acq_func='EI',\n",
        "                              n_jobs=-1,\n",
        "                              verbose=1,\n",
        "                              x0=default_parameters,random_state=42  )\n",
        "  plot_convergence(gp_result)\n",
        "\n",
        "  search_result.x\n",
        "  space.point_to_dict(gp_result.x)\n",
        "  gp_result.fun\n",
        "  sorted(zip(gp_result.func_vals, gp_result.x_iters))             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-5x83oIaPlc"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygqj6u7XZG1d"
      },
      "source": [
        "non_neural=False\n",
        "concat_model=True\n",
        "model_name = model_dir+ 'multiclass_CNN'+str(concat_model)+'.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad4f-2bbTLXl"
      },
      "source": [
        "x_train,x_val,x_test,xp_train,xp_val,xp_test,Y_train,Y_val,Y_test,encoder = load_reds_split()\n",
        "\n",
        "if concat_model==False:\n",
        "  df1=pd.DataFrame({\"x\":x_train,\"y\":Y_train})\n",
        "  df2=pd.DataFrame({\"x\":xp_train,\"y\":Y_train})\n",
        "  df2 = df2[df2.x!=\"\"]\n",
        "  df=pd.concat([df1,df2],axis=0)\n",
        "\n",
        "  dfv1=pd.DataFrame({\"x\":x_val,\"y\":Y_val})\n",
        "  dfv2=pd.DataFrame({\"x\":xp_val,\"y\":Y_val})\n",
        "  dfv2.dropna(subset=[\"x\"],inplace=True)\n",
        "  dfv2 = dfv2[dfv2.x!=\"\"]\n",
        "  dfv=pd.concat([dfv1,dfv2],axis=0)\n",
        "  x_train=df.x\n",
        "  x_val=dfv.x\n",
        "  Y_train=df.y\n",
        "  Y_val=dfv.y\n",
        "\n",
        "Y_test = encoder.transform(Y_test)\n",
        "Y_val = encoder.transform(Y_val)\n",
        "Y_train = encoder.transform(Y_train)\n",
        "y_train_labels = np.argmax(Y_train, axis =1)\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train_labels), y_train_labels)\n",
        "\n",
        "padded_sequences_x=tokenizer.texts_to_sequences(x_train)\n",
        "xtrain=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "padded_sequences_x=tokenizer.texts_to_sequences(x_val)\n",
        "xval=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "padded_sequences_x=tokenizer.texts_to_sequences(x_test)\n",
        "xtest=pad_sequences(padded_sequences_x, maxlen=max_char,padding='post', truncating='post')\n",
        "\n",
        "class_weights = {i : class_weights[i] for i in range(6)}\n",
        "\n",
        "shape=(xtrain.shape[1], vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwfNvVwORU-g"
      },
      "source": [
        "patience=2\n",
        "min_lr=0.0002\n",
        "drop_embed=.01\n",
        "activation='tanh'\n",
        "dense_units=400\n",
        "initializer='he_uniform'\n",
        "epochs=80\n",
        "kernel_sizes=[[1,50], [2,300], [3,305], [4,200], [5,250], [6,200],[7,200]]\n",
        "if concat_model:\n",
        "  patience=3\n",
        "  min_lr=0.00027\n",
        "  activation='elu'\n",
        "  drop_embed=0.02\n",
        "  dense_units=200\n",
        "  dense_activation='sigmoid',\n",
        "  kernel_sizes=[[1,239],[2,248], [3,100 ], [4,150], [5,150], [6,250],[7,200]]\n",
        "  initializer='glorot_uniform'\n",
        "  epochs=60\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor= factor ,patience=patience, min_lr= min_lr)\n",
        "\n",
        "mc = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
        "model=CNNmodel(shape,vocab_size, activation=activation, drop_embed=drop_embed, dropout=.02, dense_units=dense_units,dense_activation='sigmoid',\n",
        "              kernel_sizes=kernel_sizes,initializer=initializer,optimizer='Nadam', batch_norm=True)        \n",
        "        \n",
        "history=model.fit(xtrain, Y_train,validation_data=(xval, Y_val),initial_epoch =0, epochs =epochs,\n",
        "                      shuffle=True, batch_size = 512,verbose=1,callbacks=[mc,reduce_lr],class_weight=class_weights,\n",
        "                      workers=-1, use_multiprocessing=True)\n",
        "plot_hist(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjXxc3suWmM7"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o2jd6QLU6XI"
      },
      "source": [
        "model=load_model(model_name)\n",
        "_,x_val,x_test,_,xp_val,xp_test,_,Y_val,Y_test,encoder = load_reds_split()\n",
        "x_sbm,xp_sbm,y_sbm=load_upsbm()\n",
        "\n",
        "###########VAL\n",
        "padded_sequences_xval=tokenizer.texts_to_sequences(x_val)\n",
        "xval=pad_sequences(padded_sequences_xval, maxlen=max_char,padding='post', truncating='post')\n",
        "y_pred_val=model.predict(xval,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "df=pd.DataFrame({\"familyhead\":x_val,\"parent\":xp_val,\"religion\":Y_val,\n",
        "              \"predicted\":pd.Series(encoder.inverse_transform(y_pred_val))})\n",
        "# df.to_csv(data_dir+'val2_predictions_multiclass_CNN_concat_'+str(concat_model)+'.csv')\n",
        "\n",
        "print('REDS in SAMPLE Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(Y_val,encoder.inverse_transform(y_pred_val))))\n",
        "print(classification_report(Y_val, encoder.inverse_transform(y_pred_val),digits=4,target_names=encoder.classes_))\n",
        "conf_mat = confusion_matrix(Y_val,encoder.inverse_transform(y_pred_val))\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "############ REDSTEST\n",
        "padded_sequences_xtest=tokenizer.texts_to_sequences(x_test)\n",
        "xtest=pad_sequences(padded_sequences_xtest, maxlen=max_char,padding='post', truncating='post')\n",
        "y_pred_test=model.predict(xtest,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "df=pd.DataFrame({\"familyhead\":x_test,\"parent\":xp_test,\"religion\":pd.Series(Y_test),\n",
        "                 \"pred\":pd.Series(encoder.inverse_transform(y_pred_test))})\n",
        "# df.to_csv(data_dir+'test2_predictions_multiclass_CNN_concat_'+str(concat_model)+'.csv')\n",
        "\n",
        "print('REDS out of SAMPLE Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(Y_test,encoder.inverse_transform(y_pred_test))))\n",
        "print(classification_report(Y_test, encoder.inverse_transform(y_pred_test),digits=4,target_names=encoder.classes_))\n",
        "conf_mat = confusion_matrix(Y_test,encoder.inverse_transform(y_pred_test))\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "############ UPSBM TEST\n",
        "padded_sequences_xsbm=tokenizer.texts_to_sequences(x_sbm)\n",
        "xsbm=pad_sequences(padded_sequences_xsbm, maxlen=max_char,padding='post', truncating='post')\n",
        "y_pred_sbm=model.predict(xsbm,batch_size=1024,verbose=1,use_multiprocessing=True,workers=-1)\n",
        "df=pd.DataFrame({\"familyhead\":x_sbm,\"parent\":xp_sbm,\"religion\":y_sbm,\n",
        "              \"predicted\":pd.Series(encoder.inverse_transform(y_pred_sbm))})\n",
        "# df.to_csv(data_dir+'sbm2_predictions_multiclass_CNN_concat_'+str(concat_model)+'.csv')\n",
        "\n",
        "print('UPSBM out of SAMPLE Accuracy of classifier level 1: {:.2f}'.format(accuracy_score(y_sbm==\"Muslim\",encoder.inverse_transform(y_pred_sbm)==\"Muslim\")))\n",
        "print(classification_report(y_sbm==\"Muslim\", encoder.inverse_transform(y_pred_sbm)==\"Muslim\",digits=4,target_names=['non-muslim','muslim']))\n",
        "conf_mat = confusion_matrix(y_sbm==\"Muslim\",encoder.inverse_transform(y_pred_sbm)==\"Muslim\")\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=['non-muslim','muslim'], yticklabels=['non-muslim','muslim'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}